"""
RRF Hybrid Search ì •í™•ë„ ì¸¡ì • ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    cd server/llm_service
    python measure_search_accuracy.py

ì¸¡ì • ë°©ë²•:
1. ìë™ ì¸¡ì • (ê²°ê³¼ ìˆœìœ„ ë¹„êµ)
   - Vector Searchì™€ Hybrid Search ê²°ê³¼ì˜ ìˆœìœ„ ì°¨ì´ ì¸¡ì •
   - ê²°ê³¼ê°€ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ì§€, ìˆœìœ„ê°€ ê°œì„ ë˜ì—ˆëŠ”ì§€ ì¸¡ì •

2. ìˆ˜ë™ í‰ê°€ (Ground Truth í•„ìš”)
   - ê° ì¿¼ë¦¬ì— ëŒ€í•œ ì •ë‹µ ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„
   - Precision@K, Recall@K ê³„ì‚°

3. ì‹¤ì œ ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜
   - ì‚¬ìš©ìê°€ "ë„ì›€ë¨" ë²„íŠ¼ í´ë¦­ ë¹„ìœ¨
   - ì‘ë‹µ ë§Œì¡±ë„ ì ìˆ˜
"""

import asyncio
import os
import sys
import time
import json
import hashlib
from pathlib import Path
from typing import List, Dict, Set, Tuple
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
env_paths = [
    Path(__file__).parent.parent.parent / ".env",
    Path(__file__).parent.parent / ".env",
    Path(__file__).parent / ".env",
]

for env_path in env_paths:
    if env_path.exists():
        load_dotenv(env_path)
        break

sys.path.insert(0, str(Path(__file__).parent.parent))

from services.vector_service import VectorService
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class SearchAccuracyMeasurer:
    """ê²€ìƒ‰ ì •í™•ë„ ì¸¡ì • í´ë˜ìŠ¤"""
    
    def __init__(self, vector_service: VectorService):
        self.vector_service = vector_service
    
    def calculate_precision_at_k(self, retrieved: List[str], relevant: Set[str], k: int) -> float:
        """Precision@K ê³„ì‚°
        
        Args:
            retrieved: ê²€ìƒ‰ëœ ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸ (ìˆœìœ„ ìˆœ)
            relevant: ì •ë‹µ ë¬¸ì„œ ID ì§‘í•©
            k: ìƒìœ„ Kê°œë§Œ ê³ ë ¤
        
        Returns:
            Precision@K ê°’ (0.0 ~ 1.0)
        """
        if k == 0:
            return 0.0
        
        top_k = retrieved[:k]
        relevant_retrieved = sum(1 for doc_id in top_k if doc_id in relevant)
        return relevant_retrieved / k
    
    def calculate_recall_at_k(self, retrieved: List[str], relevant: Set[str], k: int) -> float:
        """Recall@K ê³„ì‚°
        
        Args:
            retrieved: ê²€ìƒ‰ëœ ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸ (ìˆœìœ„ ìˆœ)
            relevant: ì •ë‹µ ë¬¸ì„œ ID ì§‘í•©
            k: ìƒìœ„ Kê°œë§Œ ê³ ë ¤
        
        Returns:
            Recall@K ê°’ (0.0 ~ 1.0)
        """
        if len(relevant) == 0:
            return 0.0
        
        top_k = retrieved[:k]
        relevant_retrieved = sum(1 for doc_id in top_k if doc_id in relevant)
        return relevant_retrieved / len(relevant)
    
    def calculate_mrr(self, retrieved: List[str], relevant: Set[str]) -> float:
        """Mean Reciprocal Rank (MRR) ê³„ì‚°
        
        Args:
            retrieved: ê²€ìƒ‰ëœ ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸ (ìˆœìœ„ ìˆœ)
            relevant: ì •ë‹µ ë¬¸ì„œ ID ì§‘í•©
        
        Returns:
            MRR ê°’ (0.0 ~ 1.0)
        """
        for rank, doc_id in enumerate(retrieved, start=1):
            if doc_id in relevant:
                return 1.0 / rank
        return 0.0
    
    def calculate_rank_improvement(
        self, 
        vector_results: List[Dict], 
        hybrid_results: List[Dict]
    ) -> Dict[str, float]:
        """ê²°ê³¼ ìˆœìœ„ ê°œì„ ë„ ì¸¡ì •
        
        ê°™ì€ ë¬¸ì„œê°€ Vector Searchì™€ Hybrid Searchì—ì„œ 
        ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ ìˆœìœ„ì— ìˆëŠ”ì§€ ì¸¡ì •
        
        Returns:
            {
                'avg_rank_improvement': í‰ê·  ìˆœìœ„ ê°œì„ ë„,
                'improved_count': ìˆœìœ„ê°€ ê°œì„ ëœ ë¬¸ì„œ ìˆ˜,
                'worsened_count': ìˆœìœ„ê°€ ì•…í™”ëœ ë¬¸ì„œ ìˆ˜,
                'same_count': ìˆœìœ„ê°€ ë™ì¼í•œ ë¬¸ì„œ ìˆ˜
            }
        """
        # ë¬¸ì„œ í…ìŠ¤íŠ¸ë¥¼ í‚¤ë¡œ ì‚¬ìš© (IDê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ)
        vector_ranks = {r['text']: rank for rank, r in enumerate(vector_results, start=1)}
        hybrid_ranks = {r['text']: rank for rank, r in enumerate(hybrid_results, start=1)}
        
        # ê³µí†µ ë¬¸ì„œ ì°¾ê¸°
        common_docs = set(vector_ranks.keys()) & set(hybrid_ranks.keys())
        
        if not common_docs:
            return {
                'avg_rank_improvement': 0.0,
                'improved_count': 0,
                'worsened_count': 0,
                'same_count': 0,
                'total_common': 0
            }
        
        rank_diffs = []
        improved = 0
        worsened = 0
        same = 0
        
        for doc_text in common_docs:
            vector_rank = vector_ranks[doc_text]
            hybrid_rank = hybrid_ranks[doc_text]
            diff = vector_rank - hybrid_rank  # ì–‘ìˆ˜ë©´ ê°œì„ , ìŒìˆ˜ë©´ ì•…í™”
            
            rank_diffs.append(diff)
            if diff > 0:
                improved += 1
            elif diff < 0:
                worsened += 1
            else:
                same += 1
        
        avg_improvement = sum(rank_diffs) / len(rank_diffs) if rank_diffs else 0.0
        
        return {
            'avg_rank_improvement': avg_improvement,
            'improved_count': improved,
            'worsened_count': worsened,
            'same_count': same,
            'total_common': len(common_docs)
        }
    
    def calculate_result_diversity(
        self, 
        vector_results: List[Dict], 
        hybrid_results: List[Dict]
    ) -> Dict[str, float]:
        """ê²°ê³¼ ë‹¤ì–‘ì„± ì¸¡ì •
        
        Vector Searchì™€ Hybrid Searchê°€ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ”ì§€ ì¸¡ì •
        
        Returns:
            {
                'overlap_ratio': ê²¹ì¹˜ëŠ” ê²°ê³¼ ë¹„ìœ¨,
                'unique_vector': Vectorë§Œ ìˆëŠ” ê²°ê³¼ ìˆ˜,
                'unique_hybrid': Hybridë§Œ ìˆëŠ” ê²°ê³¼ ìˆ˜
            }
        """
        vector_texts = {r['text'][:100] for r in vector_results}  # ì²˜ìŒ 100ìë¡œ ë¹„êµ
        hybrid_texts = {r['text'][:100] for r in hybrid_results}
        
        total_unique = len(vector_texts | hybrid_texts)
        overlap = len(vector_texts & hybrid_texts)
        
        if total_unique == 0:
            return {
                'overlap_ratio': 0.0,
                'unique_vector': 0,
                'unique_hybrid': 0
            }
        
        return {
            'overlap_ratio': overlap / total_unique,
            'unique_vector': len(vector_texts - hybrid_texts),
            'unique_hybrid': len(hybrid_texts - vector_texts)
        }
    
    async def measure_single_query(
        self, 
        query: str, 
        top_k: int = 5,
        ground_truth: Set[str] = None
    ) -> Dict:
        """ë‹¨ì¼ ì¿¼ë¦¬ì— ëŒ€í•œ ì •í™•ë„ ì¸¡ì •"""
        
        # Vector Search
        start_time = time.time()
        vector_results = await self.vector_service.search_similar_documents(
            query, top_k=top_k, use_hybrid=False
        )
        vector_time = time.time() - start_time
        
        # Hybrid Search
        start_time = time.time()
        hybrid_results = await self.vector_service.search_similar_documents(
            query, top_k=top_k, use_hybrid=True
        )
        hybrid_time = time.time() - start_time
        
        # ê¸°ë³¸ ì¸¡ì •ê°’
        measurements = {
            'query': query,
            'vector_time': vector_time,
            'hybrid_time': hybrid_time,
            'time_overhead': hybrid_time - vector_time,
            'vector_count': len(vector_results),
            'hybrid_count': len(hybrid_results),
        }
        
        # ìˆœìœ„ ê°œì„ ë„ ì¸¡ì •
        rank_improvement = self.calculate_rank_improvement(vector_results, hybrid_results)
        # í‚¤ ì´ë¦„ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (rank_ ì ‘ë‘ì‚¬ ì œê±°)
        measurements.update(rank_improvement)
        
        # ê²°ê³¼ ë‹¤ì–‘ì„± ì¸¡ì •
        diversity = self.calculate_result_diversity(vector_results, hybrid_results)
        # í‚¤ ì´ë¦„ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (diversity_ ì ‘ë‘ì‚¬ ì œê±°)
        measurements.update(diversity)
        
        # ë””ë²„ê¹…: ì‹¤ì œ ê²°ê³¼ ë¹„êµ ì •ë³´ ì¶”ê°€
        measurements['vector_result_texts'] = [r['text'][:50] for r in vector_results[:3]]
        measurements['hybrid_result_texts'] = [r['text'][:50] for r in hybrid_results[:3]]
        
        # Ground Truthê°€ ìˆìœ¼ë©´ Precision/Recall ê³„ì‚°
        if ground_truth:
            # ë¬¸ì„œ ID ì¶”ì¶œ (metadataì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ í…ìŠ¤íŠ¸ í•´ì‹œ ì‚¬ìš© - ì¼ê´€ëœ í•´ì‹œ)
            def get_doc_id(result):
                text = result['text']
                return result.get('metadata', {}).get('id', f"doc_{int(hashlib.md5(text.encode()).hexdigest(), 16) % 1000000}")
            
            vector_ids = [get_doc_id(r) for r in vector_results]
            hybrid_ids = [get_doc_id(r) for r in hybrid_results]
            
            # Precision@K, Recall@K ê³„ì‚°
            for k in [1, 3, 5]:
                if k <= top_k:
                    measurements[f'vector_precision@{k}'] = self.calculate_precision_at_k(
                        vector_ids, ground_truth, k
                    )
                    measurements[f'hybrid_precision@{k}'] = self.calculate_precision_at_k(
                        hybrid_ids, ground_truth, k
                    )
                    measurements[f'vector_recall@{k}'] = self.calculate_recall_at_k(
                        vector_ids, ground_truth, k
                    )
                    measurements[f'hybrid_recall@{k}'] = self.calculate_recall_at_k(
                        hybrid_ids, ground_truth, k
                    )
            
            # MRR ê³„ì‚°
            measurements['vector_mrr'] = self.calculate_mrr(vector_ids, ground_truth)
            measurements['hybrid_mrr'] = self.calculate_mrr(hybrid_ids, ground_truth)
        
        return measurements
    
    async def measure_batch(
        self, 
        test_queries: List[Tuple[str, Set[str]]],
        top_k: int = 5
    ) -> Dict:
        """ì—¬ëŸ¬ ì¿¼ë¦¬ì— ëŒ€í•œ ë°°ì¹˜ ì¸¡ì •
        
        Args:
            test_queries: [(query, ground_truth_set), ...] ë¦¬ìŠ¤íŠ¸
            top_k: ìƒìœ„ Kê°œ ê²°ê³¼ë§Œ ê³ ë ¤
        
        Returns:
            ì „ì²´ í†µê³„ ê²°ê³¼
        """
        all_measurements = []
        
        print(f"\nğŸ“Š {len(test_queries)}ê°œ ì¿¼ë¦¬ ì¸¡ì • ì‹œì‘...")
        print("=" * 60)
        
        for i, (query, ground_truth) in enumerate(test_queries, 1):
            print(f"\n[{i}/{len(test_queries)}] '{query}'")
            try:
                measurement = await self.measure_single_query(query, top_k, ground_truth)
                all_measurements.append(measurement)
                
                # ê°„ë‹¨í•œ ì§„í–‰ ìƒí™© ì¶œë ¥
                rank_improvement = measurement.get('avg_rank_improvement', 0)
                total_common = measurement.get('total_common', 0)
                improved = measurement.get('improved_count', 0)
                worsened = measurement.get('worsened_count', 0)
                same = measurement.get('same_count', 0)
                
                print(f"   ìˆœìœ„ ê°œì„ ë„: {rank_improvement:+.2f} (ê³µí†µ ë¬¸ì„œ: {total_common}ê°œ)")
                if total_common > 0:
                    print(f"   ê°œì„ : {improved}ê°œ, ì•…í™”: {worsened}ê°œ, ë™ì¼: {same}ê°œ")
                else:
                    print(f"   âš ï¸  ê³µí†µ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤ (ê²°ê³¼ê°€ ì™„ì „íˆ ë‹¤ë¦„)")
                
                print(f"   ì‹œê°„: ğŸŸ¢ Hybrid {measurement['hybrid_time']:.3f}s (ë¦¬íŒ©í† ë§ í›„), "
                      f"ğŸ”µ Vector {measurement['vector_time']:.3f}s (ë¦¬íŒ©í† ë§ ì „)")
                
                # ë°ì´í„° ë¶€ì¡± ê²½ê³ 
                if measurement['vector_count'] < top_k:
                    print(f"   âš ï¸  ë°ì´í„° ë¶€ì¡±: {measurement['vector_count']}ê°œ ë¬¸ì„œë§Œ ì¡´ì¬ (ìš”ì²­: {top_k}ê°œ)")
            except Exception as e:
                logger.error(f"ì¿¼ë¦¬ '{query}' ì¸¡ì • ì‹¤íŒ¨: {e}")
                continue
        
        # í†µê³„ ê³„ì‚°
        if not all_measurements:
            return {'error': 'ì¸¡ì • ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.'}
        
        stats = self._calculate_statistics(all_measurements)
        return stats
    
    def _calculate_statistics(self, measurements: List[Dict]) -> Dict:
        """ì¸¡ì • ê²°ê³¼ í†µê³„ ê³„ì‚°"""
        stats = {
            'total_queries': len(measurements),
            'avg_vector_time': sum(m['vector_time'] for m in measurements) / len(measurements),
            'avg_hybrid_time': sum(m['hybrid_time'] for m in measurements) / len(measurements),
            'avg_time_overhead': sum(m['time_overhead'] for m in measurements) / len(measurements),
        }
        
        # ìˆœìœ„ ê°œì„ ë„ í†µê³„
        rank_improvements = [m.get('avg_rank_improvement', 0) for m in measurements]
        if rank_improvements:
            stats['avg_rank_improvement'] = sum(rank_improvements) / len(rank_improvements)
            stats['max_rank_improvement'] = max(rank_improvements)
            stats['min_rank_improvement'] = min(rank_improvements)
        
        # ê²°ê³¼ ë‹¤ì–‘ì„± í†µê³„
        diversity_ratios = [m.get('overlap_ratio', 0) for m in measurements]
        if diversity_ratios:
            stats['avg_diversity_overlap'] = sum(diversity_ratios) / len(diversity_ratios)
        
        # ë°ì´í„° ë¶€ì¡± ì—¬ë¶€ í™•ì¸ (ì‹¤ì œ ChromaDB ë¬¸ì„œ ìˆ˜ í™•ì¸)
        try:
            collection = self.vector_service.vector_store._collection
            all_data = collection.get(limit=10000)
            actual_doc_count = len(all_data.get('documents', []))
            stats['total_documents'] = actual_doc_count
            stats['data_sufficient'] = actual_doc_count >= 10  # ìµœì†Œ 10ê°œ ë¬¸ì„œ ê¶Œì¥
        except Exception:
            # í´ë°±: ì¸¡ì •ê°’ì—ì„œ ìµœëŒ€ê°’ ì‚¬ìš©
            total_docs = max([m.get('vector_count', 0) for m in measurements] + [0])
            stats['total_documents'] = total_docs
            stats['data_sufficient'] = total_docs >= 10
        
        # Ground Truth ê¸°ë°˜ ì§€í‘œ (ìˆëŠ” ê²½ìš°)
        precision_keys = [k for k in measurements[0].keys() if 'precision@' in k]
        if precision_keys:
            for key in precision_keys:
                values = [m[key] for m in measurements if key in m]
                if values:
                    stats[f'avg_{key}'] = sum(values) / len(values)
        
        recall_keys = [k for k in measurements[0].keys() if 'recall@' in k]
        if recall_keys:
            for key in recall_keys:
                values = [m[key] for m in measurements if key in m]
                if values:
                    stats[f'avg_{key}'] = sum(values) / len(values)
        
        mrr_keys = [k for k in measurements[0].keys() if 'mrr' in k]
        if mrr_keys:
            for key in mrr_keys:
                values = [m[key] for m in measurements if key in m]
                if values:
                    stats[f'avg_{key}'] = sum(values) / len(values)
        
        return stats


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸ“Š Hybrid Search ì •í™•ë„ ì¸¡ì • (ë¦¬íŒ©í† ë§ í›„)")
    print("   ë¦¬íŒ©í† ë§ ì „(Vector)ê³¼ ë¹„êµí•˜ì—¬ ì„±ëŠ¥ ì¸¡ì •")
    print("=" * 60)
    
    # VectorService ì´ˆê¸°í™”
    print("\n1ï¸âƒ£ VectorService ì´ˆê¸°í™” ì¤‘...")
    try:
        vector_service = VectorService()
        print("âœ… ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    measurer = SearchAccuracyMeasurer(vector_service)
    
    # ì¸¡ì • ëª¨ë“œ ì„ íƒ
    print("\n2ï¸âƒ£ ì¸¡ì • ëª¨ë“œ ì„ íƒ:")
    print("1. ìë™ ì¸¡ì • (ìˆœìœ„ ê°œì„ ë„ + ê²°ê³¼ ë‹¤ì–‘ì„±)")
    print("2. Ground Truth ê¸°ë°˜ ì¸¡ì • (Precision/Recall/MRR)")
    print("3. ì‚¬ìš©ì ì •ì˜ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬")
    
    mode = input("\nì„ íƒ (1, 2, ë˜ëŠ” 3, ê¸°ë³¸ê°’: 1): ").strip() or "1"
    
    if mode == "1":
        # ìë™ ì¸¡ì • ëª¨ë“œ
        test_queries = [
            ("ì•„ì´ê°€ ë°¤ì— ì ì„ ì•ˆ ììš”", set()),
            ("ìˆ˜ë©´ ë¬¸ì œ í•´ê²° ë°©ë²•", set()),
            ("ìœ¡ì•„ ì¼ì • ê´€ë¦¬", set()),
            ("ì»¤ë®¤ë‹ˆí‹° ì°¾ê¸°", set()),
            ("ì˜ˆë°©ì ‘ì¢… ì¼ì •", set()),
        ]
        
        print(f"\nğŸ“ {len(test_queries)}ê°œ ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‚¬ìš©")
        stats = await measurer.measure_batch(
            [(q, gt) for q, gt in test_queries],
            top_k=5
        )
        
    elif mode == "2":
        # Ground Truth ê¸°ë°˜ ì¸¡ì •
        print("\n2ï¸âƒ£ Ground Truth ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        # ground_truth.json íŒŒì¼ ì½ê¸°
        script_dir = Path(__file__).parent
        ground_truth_file = script_dir / "ground_truth.json"
        
        if not ground_truth_file.exists():
            print(f"âŒ Ground Truth íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ground_truth_file}")
            print(f"\nğŸ’¡ ë¨¼ì € prepare_ground_truth.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ì •ë‹µ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ì„¸ìš”:")
            print(f"   python prepare_ground_truth.py")
            return
        
        try:
            with open(ground_truth_file, 'r', encoding='utf-8') as f:
                ground_truth_data = json.load(f)
            
            print(f"âœ… Ground Truth ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(ground_truth_data)}ê°œ ì¿¼ë¦¬")
            
            # Ground Truth ë°ì´í„°ë¥¼ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            test_queries = [
                (query, set(doc_ids)) 
                for query, doc_ids in ground_truth_data.items()
            ]
            
            print(f"\nğŸ“ {len(test_queries)}ê°œ ì¿¼ë¦¬ë¡œ Precision/Recall/MRR ì¸¡ì • ì‹œì‘...")
            stats = await measurer.measure_batch(test_queries, top_k=5)
            
        except json.JSONDecodeError as e:
            print(f"âŒ Ground Truth íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return
        except Exception as e:
            print(f"âŒ Ground Truth íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            return
        
    else:
        # ì‚¬ìš©ì ì •ì˜ ì¿¼ë¦¬
        print("\nğŸ“ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ë¹ˆ ì¤„ ì…ë ¥ ì‹œ ì¢…ë£Œ):")
        queries = []
        while True:
            query = input("ì¿¼ë¦¬: ").strip()
            if not query:
                break
            queries.append((query, set()))
        
        if not queries:
            print("âš ï¸  ì…ë ¥ëœ ì¿¼ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        stats = await measurer.measure_batch(queries, top_k=5)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“Š ì¸¡ì • ê²°ê³¼ ìš”ì•½ (ë¦¬íŒ©í† ë§ í›„: Hybrid Search ê¸°ì¤€)")
    print("=" * 60)
    
    print(f"\nâœ… ì´ ì¿¼ë¦¬ ìˆ˜: {stats.get('total_queries', 0)}ê°œ")
    
    # ë°ì´í„° ë¶€ì¡± ê²½ê³ 
    total_docs = stats.get('total_documents', 0)
    if total_docs < 10:
        print(f"\nâš ï¸  ë°ì´í„° ë¶€ì¡± ê²½ê³ :")
        print(f"   í˜„ì¬ ë¬¸ì„œ ìˆ˜: {total_docs}ê°œ")
        print(f"   ê¶Œì¥ ë¬¸ì„œ ìˆ˜: ìµœì†Œ 10ê°œ ì´ìƒ (í˜„ì¬ëŠ” {total_docs}ê°œë§Œ ìˆì–´ì„œ")
        print(f"   ë¦¬íŒ©í† ë§ ì „(Vector)ê³¼ ë¦¬íŒ©í† ë§ í›„(Hybrid)ê°€ ë™ì¼í•œ ê²°ê³¼ë¥¼ ë°˜í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        print(f"\nğŸ’¡ í•´ê²° ë°©ë²•:")
        print(f"   - test_hybrid_search.pyì˜ ì˜µì…˜ 2ë¥¼ ì„ íƒí•˜ì—¬ ìƒ˜í”Œ ë°ì´í„° ì¶”ê°€")
        print(f"   - ë˜ëŠ” ì‹¤ì œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ í†µí•´ ë” ë§ì€ ë°ì´í„° ì¶”ê°€")
    
    if 'avg_vector_time' in stats:
        print(f"\nâ±ï¸  í‰ê·  ê²€ìƒ‰ ì‹œê°„:")
        print(f"   ğŸŸ¢ Hybrid Search (ë¦¬íŒ©í† ë§ í›„, í˜„ì¬ ì‚¬ìš©): {stats['avg_hybrid_time']:.3f}ì´ˆ")
        print(f"   ğŸ”µ Vector Search (ë¦¬íŒ©í† ë§ ì „, ë¹„êµìš©): {stats['avg_vector_time']:.3f}ì´ˆ")
        overhead = stats['avg_time_overhead']
        if overhead < 0:
            print(f"   âš¡ Hybridê°€ {abs(overhead):.3f}ì´ˆ ë” ë¹ ë¦„!")
        else:
            print(f"   âš ï¸  Hybridê°€ {overhead:.3f}ì´ˆ ë” ëŠë¦¼ (ë°ì´í„°ê°€ ì ì–´ì„œì¼ ìˆ˜ ìˆìŒ)")
    
    if 'avg_rank_improvement' in stats:
        print(f"\nğŸ“ˆ ìˆœìœ„ ê°œì„ ë„ (ë¦¬íŒ©í† ë§ ì „ ëŒ€ë¹„):")
        print(f"   í‰ê· : {stats['avg_rank_improvement']:+.2f}")
        print(f"   ìµœëŒ€: {stats.get('max_rank_improvement', 0):+.2f}")
        print(f"   ìµœì†Œ: {stats.get('min_rank_improvement', 0):+.2f}")
        print(f"   (ì–‘ìˆ˜ = Hybridê°€ ê°œì„ , ìŒìˆ˜ = Hybridê°€ ì•…í™”, 0 = ë™ì¼)")
    
    if 'avg_diversity_overlap' in stats:
        print(f"\nğŸ”„ ê²°ê³¼ ë‹¤ì–‘ì„±:")
        print(f"   ê²¹ì¹˜ëŠ” ê²°ê³¼ ë¹„ìœ¨: {stats['avg_diversity_overlap']:.1%}")
    
    # Precision/Recall ê²°ê³¼ ì¶œë ¥ (Hybrid Search ë©”ì¸)
    precision_keys = [k for k in stats.keys() if 'precision@' in k]
    if precision_keys:
        print(f"\nğŸ¯ Precision@K:")
        # Hybrid Search ë¨¼ì € ì¶œë ¥
        hybrid_precision = [k for k in sorted(precision_keys) if 'hybrid' in k]
        vector_precision = [k for k in sorted(precision_keys) if 'vector' in k]
        for key in hybrid_precision:
            k = key.split('@')[1]
            print(f"   ğŸŸ¢ Hybrid Search (ë¦¬íŒ©í† ë§ í›„): {stats[key]:.1%}")
        for key in vector_precision:
            k = key.split('@')[1]
            print(f"   ğŸ”µ Vector Search (ë¦¬íŒ©í† ë§ ì „, ë¹„êµìš©): {stats[key]:.1%}")
    
    recall_keys = [k for k in stats.keys() if 'recall@' in k]
    if recall_keys:
        print(f"\nğŸ“Š Recall@K:")
        # Hybrid Search ë¨¼ì € ì¶œë ¥
        hybrid_recall = [k for k in sorted(recall_keys) if 'hybrid' in k]
        vector_recall = [k for k in sorted(recall_keys) if 'vector' in k]
        for key in hybrid_recall:
            k = key.split('@')[1]
            print(f"   ğŸŸ¢ Hybrid Search (ë¦¬íŒ©í† ë§ í›„): {stats[key]:.1%}")
        for key in vector_recall:
            k = key.split('@')[1]
            print(f"   ğŸ”µ Vector Search (ë¦¬íŒ©í† ë§ ì „, ë¹„êµìš©): {stats[key]:.1%}")
    
    mrr_keys = [k for k in stats.keys() if 'mrr' in k]
    if mrr_keys:
        print(f"\nğŸ† MRR (Mean Reciprocal Rank):")
        # Hybrid Search ë¨¼ì € ì¶œë ¥
        hybrid_mrr = [k for k in sorted(mrr_keys) if 'hybrid' in k]
        vector_mrr = [k for k in sorted(mrr_keys) if 'vector' in k]
        for key in hybrid_mrr:
            print(f"   ğŸŸ¢ Hybrid Search (ë¦¬íŒ©í† ë§ í›„): {stats[key]:.3f}")
        for key in vector_mrr:
            print(f"   ğŸ”µ Vector Search (ë¦¬íŒ©í† ë§ ì „, ë¹„êµìš©): {stats[key]:.3f}")
    
    # ê°œì„ ìœ¨ ê³„ì‚°
    if 'avg_hybrid_precision@5' in stats and 'avg_vector_precision@5' in stats:
        improvement = (
            (stats['avg_hybrid_precision@5'] - stats['avg_vector_precision@5']) 
            / stats['avg_vector_precision@5'] * 100
            if stats['avg_vector_precision@5'] > 0 else 0
        )
        print(f"\nğŸš€ ë¦¬íŒ©í† ë§ í›„ ê°œì„ ìœ¨ (Precision@5): {improvement:+.1f}%")
        print(f"   (ë¦¬íŒ©í† ë§ ì „ Vector Search ëŒ€ë¹„)")
    
    print("\n" + "=" * 60)
    print("âœ… ì¸¡ì • ì™„ë£Œ!")
    print("=" * 60)
    
    # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í• ì§€ ë¬¼ì–´ë³´ê¸°
    save = input("\nê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n, ê¸°ë³¸ê°’: n): ").strip().lower()
    if save == 'y':
        from datetime import datetime
        
        filename = f"search_accuracy_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        print(f"âœ… ê²°ê³¼ ì €ì¥: {filename}")


if __name__ == "__main__":
    asyncio.run(main())

